{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# üì¶ Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import folium\n",
    "import re\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# üìç File paths\n",
    "RAW_PATH = '../data/raw/your_portal_export.csv'  # <- Replace with actual filename\n",
    "CLEAN_PATH = '../data/cleaned/food_inspections_cleaned.csv'\n",
    "MAP_PATH = '../maps/grocery_stores_chicago_map.html'\n",
    "\n",
    "# üì• Load dataset\n",
    "df = pd.read_csv(RAW_PATH)\n",
    "print(f\"‚úÖ Loaded portal export: {df.shape[0]} rows\")\n",
    "\n",
    "# ‚úÇÔ∏è Keep relevant columns\n",
    "columns_to_keep = [\n",
    "    'DBA Name', 'Address', 'City', 'State', 'Zip',\n",
    "    'Inspection Date', 'Results', 'Violations',\n",
    "    'Latitude', 'Longitude'\n",
    "]\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# üßπ Drop missing coordinates\n",
    "df = df.dropna(subset=['Latitude', 'Longitude'])\n",
    "df = df[(df['Latitude'] != '') & (df['Longitude'] != '')]\n",
    "\n",
    "# üî† Normalize key fields\n",
    "df['DBA Name'] = df['DBA Name'].str.title().str.strip()\n",
    "df['Address'] = df['Address'].str.title().str.strip()\n",
    "df['Results'] = df['Results'].str.title().str.strip()\n",
    "\n",
    "# Remove any entries still missing key info\n",
    "df = df[df['Address'].notnull() & df['DBA Name'].notnull()]\n",
    "\n",
    "# Normalize for matching\n",
    "df['DBA_NAME_CLEAN'] = (\n",
    "    df['DBA Name']\n",
    "    .str.upper()\n",
    "    .str.replace(r'[^A-Z0-9\\s]', '', regex=True)\n",
    "    .str.replace(r'\\s+', ' ', regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "df['ADDRESS_CLEAN'] = (\n",
    "    df['Address']\n",
    "    .str.upper()\n",
    "    .str.replace(r'[^A-Z0-9\\s]', '', regex=True)\n",
    "    .str.replace(r'\\s+', ' ', regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Parse inspection dates\n",
    "df['Inspection Date'] = pd.to_datetime(df['Inspection Date'], errors='coerce')\n",
    "\n",
    "# üßπ Keep only the most recent inspection per location\n",
    "df = df.sort_values(by='Inspection Date', ascending=False)\n",
    "df_deduped = df.drop_duplicates(subset=['ADDRESS_CLEAN', 'Zip'], keep='first').reset_index(drop=True)\n",
    "print(f\"üìâ Deduplicated to: {len(df_deduped)} unique locations\")\n",
    "\n",
    "# üè∑Ô∏è Define classification patterns\n",
    "grocery_keywords = [\n",
    "    'ALDI', 'JEWEL', 'MARIANO', 'WHOLE FOODS', 'FOOD MARKET',\n",
    "    'SUPERMARKET', 'TRADER JOES', 'CERMARK', 'FRESH MARKET',\n",
    "    'LOS SAUCES', 'CERMAK', 'BUFFALO FOODS', 'SUPER LEON',\n",
    "    'AL EMAAN', 'TONYS', 'PETES'\n",
    "]\n",
    "\n",
    "junk_keywords = [\n",
    "    r'7[-‚Äì\\s]?ELEVEN', 'DOLLAR', 'CIRCLE K', 'SHELL', 'MOBIL',\n",
    "    'GAS', 'CONVENIENCE', 'MINI MART', 'WALGREENS', 'CVS',\n",
    "    'BP', 'CITGO', 'EXXON', 'AMOCO'\n",
    "]\n",
    "\n",
    "# üß™ Flag store types\n",
    "df_deduped['IS_JUNK_STORE'] = df_deduped['DBA_NAME_CLEAN'].apply(\n",
    "    lambda name: any(re.search(pattern, name) for pattern in junk_keywords)\n",
    ")\n",
    "\n",
    "df_deduped['IS_REAL_GROCERY'] = df_deduped['DBA_NAME_CLEAN'].apply(\n",
    "    lambda name: any(keyword in name for keyword in grocery_keywords)\n",
    ")\n",
    "\n",
    "# üçé Add extra signal from violations\n",
    "produce_hints = ['fruit', 'vegetable', 'cut', 'salad', 'raw', 'cold', 'prep']\n",
    "df_deduped['HAS_PRODUCE_FLAG'] = df_deduped['Violations'].fillna('').str.lower().apply(\n",
    "    lambda v: any(word in v for word in produce_hints)\n",
    ")\n",
    "df_deduped['IS_REAL_GROCERY'] = df_deduped['IS_REAL_GROCERY'] | df_deduped['HAS_PRODUCE_FLAG']\n",
    "\n",
    "# üßº Remove conflicting dual-labels (prefer real grocery if both)\n",
    "conflicts = df_deduped[df_deduped['IS_REAL_GROCERY'] & df_deduped['IS_JUNK_STORE']]\n",
    "df_deduped.loc[conflicts.index, 'IS_JUNK_STORE'] = False\n",
    "\n",
    "# üíæ Save cleaned data\n",
    "os.makedirs(os.path.dirname(CLEAN_PATH), exist_ok=True)\n",
    "df_deduped.to_csv(CLEAN_PATH, index=False)\n",
    "print(f\"‚úÖ Cleaned dataset saved to: {CLEAN_PATH}\")\n",
    "\n",
    "# üó∫Ô∏è Generate map\n",
    "m = folium.Map(location=[41.8781, -87.6298], zoom_start=11)\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "for _, row in df_deduped.iterrows():\n",
    "    lat, lon = row['Latitude'], row['Longitude']\n",
    "    if pd.isna(lat) or pd.isna(lon):\n",
    "        continue\n",
    "\n",
    "    # Marker style\n",
    "    if row['IS_REAL_GROCERY']:\n",
    "        color, icon = 'green', 'shopping-cart'\n",
    "    elif row['IS_JUNK_STORE']:\n",
    "        color, icon = 'red', 'remove'\n",
    "    else:\n",
    "        color, icon = 'gray', 'info-sign'\n",
    "\n",
    "    popup = f\"{row['DBA Name']}<br>{row['Address']}<br>ZIP: {int(row['Zip']) if pd.notnull(row['Zip']) else ''}\"\n",
    "    folium.Marker(\n",
    "        location=[lat, lon],\n",
    "        popup=popup,\n",
    "        icon=folium.Icon(color=color, icon=icon)\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# üíæ Save map to file\n",
    "os.makedirs(os.path.dirname(MAP_PATH), exist_ok=True)\n",
    "m.save(MAP_PATH)\n",
    "print(f\"üó∫Ô∏è Map saved to: {MAP_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
