{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/archangel/Desktop/chi-food-access-map/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded portal export: 35681 rows\n",
      "üìâ Deduplicated to: 4503 unique locations\n",
      "‚úÖ Cleaned dataset saved to: ../data/cleaned/food_inspections_cleaned.csv\n",
      " Map saved to: ../maps/grocery_stores_chicago_map.html\n",
      "‚úÖ grocery_stores_cleaned_v1.csv saved!\n"
     ]
    }
   ],
   "source": [
    "# üß† Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import folium\n",
    "import re\n",
    "from folium.plugins import MarkerCluster\n",
    "\n",
    "# File paths\n",
    "RAW_PATH = '../data/raw/food_inspections_filtered.csv'  # ‚Üê filtered export from Chicago Data Portal\n",
    "CLEAN_PATH = '../data/cleaned/food_inspections_cleaned.csv'\n",
    "MAP_PATH = '../docs/grocery_stores_chicago_map.html'\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(RAW_PATH)\n",
    "print(f\"‚úÖ Loaded portal export: {df.shape[0]} rows\")\n",
    "\n",
    "# Keep only relevant columns\n",
    "columns_to_keep = [\n",
    "    'DBA Name', 'Address', 'City', 'State', 'Zip',\n",
    "    'Inspection Date', 'Results', 'Violations',\n",
    "    'Latitude', 'Longitude'\n",
    "]\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "# Drop rows with missing or invalid coordinates\n",
    "df = df.dropna(subset=['Latitude', 'Longitude'])\n",
    "df = df[(df['Latitude'] != '') & (df['Longitude'] != '')]\n",
    "\n",
    "# Normalize key fields\n",
    "df['DBA Name'] = df['DBA Name'].str.title().str.strip()\n",
    "df['Address'] = df['Address'].str.title().str.strip()\n",
    "df['Results'] = df['Results'].str.title().str.strip()\n",
    "\n",
    "# Remove any leftover rows missing name/address\n",
    "df = df[df['Address'].notnull() & df['DBA Name'].notnull()]\n",
    "\n",
    "# Normalize for pattern matching\n",
    "df['DBA_NAME_CLEAN'] = (\n",
    "    df['DBA Name']\n",
    "    .str.upper()\n",
    "    .str.replace(r'[^A-Z0-9\\s]', '', regex=True)\n",
    "    .str.replace(r'\\s+', ' ', regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "df['ADDRESS_CLEAN'] = (\n",
    "    df['Address']\n",
    "    .str.upper()\n",
    "    .str.replace(r'[^A-Z0-9\\s]', '', regex=True)\n",
    "    .str.replace(r'\\s+', ' ', regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "# Parse inspection dates\n",
    "df['Inspection Date'] = pd.to_datetime(df['Inspection Date'], errors='coerce')\n",
    "\n",
    "# Deduplicate: keep only most recent inspection per address + zip\n",
    "df = df.sort_values(by='Inspection Date', ascending=False)\n",
    "df_deduped = df.drop_duplicates(subset=['ADDRESS_CLEAN', 'Zip']).reset_index(drop=True)\n",
    "print(f\"üìâ Deduplicated to: {df_deduped.shape[0]} unique locations\")\n",
    "\n",
    "# Define grocery and junk keyword lists\n",
    "grocery_keywords = [\n",
    "    'ALDI', 'JEWEL', 'MARIANO', 'WHOLE FOODS', 'FOOD MARKET',\n",
    "    'SUPERMARKET', 'TRADER JOES', 'CERMARK', 'FRESH MARKET',\n",
    "    'LOS SAUCES', 'CERMAK', 'BUFFALO FOODS', 'SUPER LEON',\n",
    "    'AL EMAAN', 'TONYS', 'PETES', 'AMAZON GO', 'BOCKWINKELS', 'FOOD 4 LESS MIDWEST',\n",
    "\n",
    "    # Flag real unique grocery stores\n",
    "    'DELOBIAN', '5826 DIVISION FOOD', 'AFRICAN', 'AUSTIN PRODUCE',\n",
    "    'AVENUE FOODS', 'AVENUE GROCERY MART', 'BROOKLYN MART', 'BROKLYN DELI MART',\n",
    "    'BY THE HAND', 'BEST FOOD', 'CHICAGO AVE FOOD AND DELI',\n",
    "    'CICERO CORNER FOOD', 'CICERO BIG MART', '233 MARKET', 'MARKET', \n",
    "    'CN DISCOUNTS, INC', 'F0 GROUP LLC', 'DISCOUNT STORE', 'EAGLE DISCOUNT',\n",
    "    'CORCORAN GROCERY', '225 MARKET', 'AMAZON RETAIL', 'FRAMSTEAD',\n",
    "    'BARGAIN FOOD', 'ANDYS', 'GROCERIES MI CUBA', 'GREENLINE FOOD',\n",
    "    'HEAVEN ELEVEN', 'HUNAID FOODS', 'JIMMYS FOOD', 'JORDAN DISCOUNT'\n",
    "]\n",
    "\n",
    "junk_keywords = [\n",
    "    r'7[-‚Äì\\s]?ELEVEN', 'DOLLAR', 'CIRCLE K', 'SHELL', 'MOBIL',\n",
    "    'GAS', 'CONVENIENCE', 'MINI MART', 'WALGREENS', 'CVS',\n",
    "    'BP', 'CITGO', 'EXXON', 'AMOCO', 'LIQUOR', 'TOBACCO', 'FOOD MART', 'CORNER STORE',\n",
    "    'SHOP', 'QUICK MART', 'EXPRESS', 'FAST STOP',\n",
    "\n",
    "    # Custom junk patterns - CLEANED\n",
    "    'AMSTAR', 'AMZ', 'ENTERPRISES', '24 FOODS', 'AMERICANA SUBMARINE', 'ARCO',\n",
    "    'D AND D FOODMART', 'DAY NIGHT', 'A P NEWSSTAND', 'BOND DRUG COMPANY', 'CANDYALITY',\n",
    "    'DESI MART', 'C B TABBACO', '76', 'BIG CORNER FOOD', 'CHICAGOS MEATERY',\n",
    "    'FANNIE MAY', 'DELI MART', 'CIRCLE S MART INC.', 'EASY SNACK',\n",
    "    'FOXTROT', '4 GUYS', '7 DAYS', 'A G GROCERY', 'ADAM FOOD', 'AUSTIN FOOD AND BEVERAGE',\n",
    "    'NEWSTAND', 'ROCKET', 'AUGSTA', 'CRISTIS', 'DEL REY TORTILLERIA', 'CARRILLOS', 'BIRDIE',\n",
    "    'FOOD, WINE', 'SPIRIT', 'CHAVA', 'CREMERIA', 'CHIVO CHIQUITA',\n",
    "    'BRENDA NUTRITIONS', 'BOJORQUEZ', 'CENT', 'F.A.M.', 'CARNICERIA', 'ARAMANDOS', 'FALCON 300',\n",
    "    'HARRISON STREET',\n",
    "\n",
    "    # Additional Austin unclassified patterns\n",
    "    '123 FOOD MARKET', '365 FOODMART', '4 BROTHERS FOOD MART', '5826 DIVISION FOOD',\n",
    "    'ADAM & ARAM', 'AFRICAN 09', 'APPLE FOOD MART', 'AUGUSTA FOOD', 'AVENUE GROCERY MART',\n",
    "    'AYMAN FOOD MARKET', 'BARGAIN FOOD', \"BIG MOE'S\", 'BOBOS MARKET', 'BROOKLYN DELI MART'\n",
    "]\n",
    "\n",
    "\n",
    "# Flag junk and grocery stores\n",
    "df_deduped['IS_JUNK_STORE'] = df_deduped['DBA_NAME_CLEAN'].apply(\n",
    "    lambda name: any(re.search(pattern, name) for pattern in junk_keywords)\n",
    ")\n",
    "\n",
    "df_deduped['IS_REAL_GROCERY'] = df_deduped['DBA_NAME_CLEAN'].apply(\n",
    "    lambda name: any(keyword in name for keyword in grocery_keywords)\n",
    ")\n",
    "\n",
    "# Add extra signal from violations\n",
    "produce_hints = ['fruit', 'vegetable', 'cut', 'salad', 'raw', 'cold', 'prep']\n",
    "df_deduped['HAS_PRODUCE_FLAG'] = df_deduped['Violations'].fillna('').str.lower().apply(\n",
    "    lambda v: any(word in v for word in produce_hints)\n",
    ")\n",
    "df_deduped['IS_REAL_GROCERY'] = df_deduped['IS_REAL_GROCERY'] | df_deduped['HAS_PRODUCE_FLAG']\n",
    "\n",
    "# Remove conflicting labels (prioritize real grocery)\n",
    "conflicts = df_deduped[df_deduped['IS_REAL_GROCERY'] & df_deduped['IS_JUNK_STORE']]\n",
    "df_deduped.loc[conflicts.index, 'IS_JUNK_STORE'] = False\n",
    "\n",
    "# Save cleaned dataset\n",
    "os.makedirs(os.path.dirname(CLEAN_PATH), exist_ok=True)\n",
    "df_deduped.to_csv(CLEAN_PATH, index=False)\n",
    "print(f\"‚úÖ Cleaned dataset saved to: {CLEAN_PATH}\")\n",
    "\n",
    "# Build interactive map\n",
    "m = folium.Map(location=[41.8781, -87.6298], zoom_start=11)\n",
    "marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "for _, row in df_deduped.iterrows():\n",
    "    lat, lon = row['Latitude'], row['Longitude']\n",
    "    if pd.isna(lat) or pd.isna(lon):\n",
    "        continue\n",
    "\n",
    "    if row['IS_REAL_GROCERY']:\n",
    "        color, icon = 'green', 'shopping-cart'\n",
    "    elif row['IS_JUNK_STORE']:\n",
    "        color, icon = 'red', 'remove'\n",
    "    else:\n",
    "        color, icon = 'gray', 'info-sign'\n",
    "\n",
    "    popup = f\"{row['DBA Name']}<br>{row['Address']}<br>ZIP: {int(row['Zip']) if pd.notnull(row['Zip']) else ''}\"\n",
    "    folium.Marker(\n",
    "        location=[lat, lon],\n",
    "        popup=popup,\n",
    "        icon=folium.Icon(color=color, icon=icon)\n",
    "    ).add_to(marker_cluster)\n",
    "\n",
    "# Save map to file\n",
    "os.makedirs(os.path.dirname(MAP_PATH), exist_ok=True)\n",
    "m.save(MAP_PATH)\n",
    "print(f\" Map saved to: {MAP_PATH}\")\n",
    "\n",
    "# Save labeled + deduped grocery dataset for analysis/visualization\n",
    "df_deduped.to_csv('../data/cleaned/grocery_stores_cleaned_v1.csv', index=False)\n",
    "print(\"‚úÖ grocery_stores_cleaned_v1.csv saved!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
